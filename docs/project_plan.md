Sprint One: Create and execute process to scrape information from Google Scholar
Lu and Ehsan
Due: March 6

As discussed in March 2 project meeting.  In sprint 2, we will run clustering analysis to identify the right core nodes for the backbone of the knowledge graph to organize the data. The process at this point may scrape just keywords and executive summaries. The .py file to execute the data scrape to be stored in src folder in the git repository.


Sprint Two: Perform clustering analysis on the Google Scholar information
Everyone 
Due: March 9

As discussed in March 2 project meeting.  Everyone picks a couple of algorhithms to test on the data that's been collected and adds .py file to src folder and visualizations to the figs folder.  We will pick the best algorhithm for clustering in our March 9 meeting.

Additional spring planning to come!
