**Sprint One: Create and execute process to scrape information from Google Scholar**

Lu and Ehsan 

Due: March 6 (if that is too soon we can move it)

As discussed in March 2 project meeting. The process at this point may scrape just keywords and executive summaries. The .py file to execute the data scrape to be stored in src folder in the git repository.  In sprint 2, we will run clustering analysis to identify the right core nodes for the backbone of the knowledge graph to organize the data. 



**Sprint Two: Perform clustering analysis on the Google Scholar information**

Everyone 

Due: March 9

As discussed in March 2 project meeting. Everyone picks a couple of algorithms to test on the data that's been collected and adds .py file to src folder and visualizations to the figs folder from that analysis.  We will pick the best algorithm for clustering in our March 9 meeting.



Additional sprint planning to come!
