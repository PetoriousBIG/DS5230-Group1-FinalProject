{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaI-lU-LO_HC"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZlO0Zb4WN1m"
      },
      "source": [
        "# **Data Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "okUUeQXNL06z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file = pd.read_csv(\"Google Scholar Resources.csv\")\n",
        "\n",
        "# Ensure 'Citations' column is numeric (in case it was read as string)\n",
        "file['Citations'] = pd.to_numeric(file['Citations'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing citation values\n",
        "file = file.dropna(subset=['Citations'])\n",
        "\n",
        "# Calculate the 70th percentile (so top 30% is above this)\n",
        "threshold = file['Citations'].quantile(0.70)\n",
        "\n",
        "# Filter rows with citation count higher than the threshold\n",
        "df = file[file['Citations'] > threshold]\n",
        "\n",
        "# Optional: reset index\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qhuzZ0SreCV",
        "outputId": "91f5a8af-9993-45fa-a5a7-e0989f0247ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Title', 'Abstract', 'Authors', 'Year', 'URL', 'Citations', 'Journal'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2W4gkzbwrhzI",
        "outputId": "0af77b9f-9024-42e8-8439-0e979e56ebaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Title  \\\n",
              "0  High-performance medicine: the convergence of ...   \n",
              "1  Explainable Artificial Intelligence (XAI): Con...   \n",
              "2  Explanation in Artificial Intelligence: Insigh...   \n",
              "3  Sparks of Artificial General Intelligence: Ear...   \n",
              "4  Peeking Inside the Black-Box: A Survey on Expl...   \n",
              "\n",
              "                                            Abstract  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3  Artificial intelligence (AI) researchers have ...   \n",
              "4  At the dawn of the fourth industrial revolutio...   \n",
              "\n",
              "                                             Authors  Year  \\\n",
              "0                                           E. Topol  2019   \n",
              "1  Alejandro Barredo Arrieta, Natalia DÃ­az RodrÃ­g...  2019   \n",
              "2                                         Tim Miller  2017   \n",
              "3  SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen ...  2023   \n",
              "4                            Amina Adadi, M. Berrada  2018   \n",
              "\n",
              "                                                 URL  Citations  \\\n",
              "0  https://www.semanticscholar.org/paper/f134abea...       4040   \n",
              "1  https://www.semanticscholar.org/paper/530a059c...       5841   \n",
              "2  https://www.semanticscholar.org/paper/e89dfa30...       4081   \n",
              "3  https://www.semanticscholar.org/paper/8dbd5746...       2793   \n",
              "4  https://www.semanticscholar.org/paper/21dff47a...       3662   \n",
              "\n",
              "           Journal  \n",
              "0  Nature Medicine  \n",
              "1      Inf. Fusion  \n",
              "2   Artif. Intell.  \n",
              "3            ArXiv  \n",
              "4      IEEE Access  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f609622-4a29-4856-aa80-609dbc98944e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Year</th>\n",
              "      <th>URL</th>\n",
              "      <th>Citations</th>\n",
              "      <th>Journal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>High-performance medicine: the convergence of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>E. Topol</td>\n",
              "      <td>2019</td>\n",
              "      <td>https://www.semanticscholar.org/paper/f134abea...</td>\n",
              "      <td>4040</td>\n",
              "      <td>Nature Medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Explainable Artificial Intelligence (XAI): Con...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Alejandro Barredo Arrieta, Natalia DÃ­az RodrÃ­g...</td>\n",
              "      <td>2019</td>\n",
              "      <td>https://www.semanticscholar.org/paper/530a059c...</td>\n",
              "      <td>5841</td>\n",
              "      <td>Inf. Fusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Explanation in Artificial Intelligence: Insigh...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Tim Miller</td>\n",
              "      <td>2017</td>\n",
              "      <td>https://www.semanticscholar.org/paper/e89dfa30...</td>\n",
              "      <td>4081</td>\n",
              "      <td>Artif. Intell.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sparks of Artificial General Intelligence: Ear...</td>\n",
              "      <td>Artificial intelligence (AI) researchers have ...</td>\n",
              "      <td>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen ...</td>\n",
              "      <td>2023</td>\n",
              "      <td>https://www.semanticscholar.org/paper/8dbd5746...</td>\n",
              "      <td>2793</td>\n",
              "      <td>ArXiv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Peeking Inside the Black-Box: A Survey on Expl...</td>\n",
              "      <td>At the dawn of the fourth industrial revolutio...</td>\n",
              "      <td>Amina Adadi, M. Berrada</td>\n",
              "      <td>2018</td>\n",
              "      <td>https://www.semanticscholar.org/paper/21dff47a...</td>\n",
              "      <td>3662</td>\n",
              "      <td>IEEE Access</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f609622-4a29-4856-aa80-609dbc98944e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5f609622-4a29-4856-aa80-609dbc98944e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5f609622-4a29-4856-aa80-609dbc98944e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb34e275-34ba-43b6-b0ee-9025d1717672\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb34e275-34ba-43b6-b0ee-9025d1717672')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb34e275-34ba-43b6-b0ee-9025d1717672 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30630,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21144,\n        \"samples\": [\n          \"A survey on indexing techniques for big data: taxonomy and performance evaluation\",\n          \"Approximations for Binary Gaussian Process Classification\",\n          \"Comparisons among clustering techniques for electricity customer classification\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17189,\n        \"samples\": [\n          \"Today\\u2019s telecommunication networks have become sources of enormous amounts of widely heterogeneous data. This information can be retrieved from network traffic traces, network alarms, signal quality indicators, users\\u2019 behavioral data, etc. Advanced mathematical tools are required to extract meaningful information from these data and take decisions pertaining to the proper functioning of the networks from the network-generated data. Among these mathematical tools, machine learning (ML) is regarded as one of the most promising methodological approaches to perform network-data analysis and enable automated network self-configuration and fault management. The adoption of ML techniques in the field of optical communication networks is motivated by the unprecedented growth of network complexity faced by optical networks in the last few years. Such complexity increase is due to the introduction of a huge number of adjustable and interdependent system parameters (e.g., routing configurations, modulation format, symbol rate, coding schemes, etc.) that are enabled by the usage of coherent transmission/reception technologies, advanced digital signal processing, and compensation of nonlinear effects in optical fiber propagation. In this paper we provide an overview of the application of ML to optical communications and networking. We classify and survey relevant literature dealing with the topic, and we also provide an introductory tutorial on ML for researchers and practitioners interested in this field. Although a good number of research papers have recently appeared, the application of ML to optical networks is still in its infancy: to stimulate further work in this area, we conclude this paper proposing new possible research directions.\",\n          \"This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \\\"spectral complexity\\\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.\",\n          \"Pretraining sentence encoders with language modeling and related unsupervised tasks has recently been shown to be very effective for language understanding tasks. By supplementing language model-style pretraining with further training on data-rich supervised tasks, such as natural language inference, we obtain additional performance improvements on the GLUE benchmark. Applying supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of 81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over BERT. We also observe reduced variance across random restarts in this setting. Our approach yields similar improvements when applied to ELMo (Peters et al., 2018a) and Radford et al. (2018)'s model. In addition, the benefits of supplementary training are particularly pronounced in data-constrained regimes, as we show in experiments with artificially limited training data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20496,\n        \"samples\": [\n          \"P. Blainey, G. Luo, S C Kou, Walter F. Mangel, G. Verdine, B. Bagchi, X. S. Xie\",\n          \"Chao Yu, Jiming Liu, S. Nemati\",\n          \"Alison M Darcy, A. Louie, L. Roberts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 2005,\n        \"max\": 2025,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          2019,\n          2008,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21196,\n        \"samples\": [\n          \"https://www.semanticscholar.org/paper/3af7252f7e3ca8e9bc8f45e6cbf567b10ecb5d95\",\n          \"https://www.semanticscholar.org/paper/379f9711c44739d6bb96efe4d530c6d4a20c4924\",\n          \"https://www.semanticscholar.org/paper/67216c49d031134a6554d050ab53440014263c20\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Citations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3604,\n        \"min\": 160,\n        \"max\": 185755,\n        \"num_unique_values\": 2649,\n        \"samples\": [\n          821,\n          2965,\n          14728\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Journal\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4096,\n        \"samples\": [\n          \"Higher Education\",\n          \"Social Sciences\",\n          \"SC21: International Conference for High Performance Computing, Networking, Storage and Analysis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f-fVlbC7iPd",
        "outputId": "0a447c49-42cd-43ce-e05e-5aca9f2439d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30630, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "c7OcxSxV8Dok",
        "outputId": "0684c58e-a880-4713-8f6f-83e1c80d35ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Determine the best text column to use\n",
        "text_column = \"Abstract\" if \"Abstract\" in df.columns else \"Title\"\n",
        "text_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y_M16xNyr6az"
      },
      "outputs": [],
      "source": [
        "df[\"Abstract\"] = df[\"Abstract\"].fillna(df[\"Title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq08Q9hT2vNM"
      },
      "source": [
        "# **Text Embedding Using Bert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cs6-MVvkr-6u"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load Sentence-BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY9WJ0P38QX0",
        "outputId": "14daacbf-bbaf-4aba-a745-ff6f7975bc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Embeddings shape: (30630, 384)\n",
            "Reduced embeddings shape: (30630, 64)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the saved NumPy arrays\n",
        "import numpy as np\n",
        "\n",
        "embeddings = np.load(\"/content/drive/MyDrive/embeddings.npy\")\n",
        "reduced_embeddings = np.load(\"/content/drive/MyDrive/reduced_embeddings.npy\")\n",
        "\n",
        "#check the shape\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "print(\"Reduced embeddings shape:\", reduced_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06BONwYIsVCH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Preprocess text\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        text = text.strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df[text_column] = df[text_column].fillna(\"\").apply(clean_text)\n",
        "\n",
        "# Generate embeddings efficiently\n",
        "print(\"Generating text embeddings...\")\n",
        "embeddings = model.encode(df[text_column].tolist(), batch_size=128, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RBr2GtWsukU"
      },
      "outputs": [],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jE5C1tW2AQ6"
      },
      "source": [
        "# **Dimantiality Reduction Using Autoencoder**\n",
        "We use an autoencoder neural network to perform unsupervised dimensionality reduction and anomaly detection on high-dimensional embeddings.The autoencoder learns to compress each embedding into a lower-dimensional latent representation (via the bottleneck layer) and then reconstruct it back to its original form. By minimizing the reconstruction error during training, the model captures the most important features of the data. Once trained, each document's reconstruction error is computed, and those with the highest errors (the top 5%) are identified as poorly reconstructed samples which may represent outliers, unusual structure, or noise.\n",
        "\n",
        "The goal is to detect and optionally filter out samples that deviate significantly from the learned structure of the dataset. This improves the quality of downstream tasks in this project inclusing clustering, semantic search, or topic modeling by reducing the influence of noisy or inconsistent data. The benefits include cleaner datasets, better-defined clusters, and the ability to identify edge cases or rare patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIJchek4rJzh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_dim = embeddings.shape[1]  # Input size equals the dimension of embeddings\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))  # Input layer\n",
        "encoded = Dense(128, activation='relu')(input_layer)  # First encoding layer\n",
        "encoded = Dense(64, activation='relu')(encoded)       # Bottleneck layer (compressed representation)\n",
        "decoded = Dense(128, activation='relu')(encoded)      # First decoding layer\n",
        "decoded = Dense(input_dim, activation='linear')(decoded)  # Output layer (reconstruct original input)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)  # Full autoencoder model\n",
        "encoder = Model(inputs=input_layer, outputs=encoded)      # Encoder model (for dimensionality reduction)\n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  # Compile with MSE loss\n",
        "\n",
        "# Train the autoencoder to reconstruct input embeddings\n",
        "autoencoder.fit(embeddings, embeddings, epochs=50, batch_size=32, shuffle=True)\n",
        "\n",
        "# Get reduced (encoded) and reconstructed embeddings\n",
        "reduced_embeddings = encoder.predict(embeddings)              # Compressed version\n",
        "reconstructed_embeddings = autoencoder.predict(embeddings)    # Output from autoencoder\n",
        "\n",
        "# Compute reconstruction error (MSE) for each sample\n",
        "reconstruction_errors = np.mean((embeddings - reconstructed_embeddings) ** 2, axis=1)\n",
        "\n",
        "# Identify samples with reconstruction error above the 95th percentile\n",
        "threshold = np.percentile(reconstruction_errors, 95)  # Top 5% error threshold\n",
        "poorly_reconstructed_indices = np.where(reconstruction_errors > threshold)[0]  # Indices of poor samples\n",
        "\n",
        "print(f\"\\nNumber of poorly reconstructed samples: {len(poorly_reconstructed_indices)}\")\n",
        "\n",
        "# Plot histogram of reconstruction errors\n",
        "plt.hist(reconstruction_errors, bins=50)\n",
        "plt.title(\"Distribution of Reconstruction Errors\")\n",
        "plt.xlabel(\"MSE Loss\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFJDwmTUrynz",
        "outputId": "7d3b1ed2-e73e-4622-a612-4d83fc9489f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30630, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "reduced_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB_ncD5V1Rx6"
      },
      "source": [
        "# **Find The Best Number Of Components For GMM Based On silhouette Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuu3Hb_xhZWx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Use the first two dimensions for visualization\n",
        "sil_scores = []\n",
        "\n",
        "# Try different numbers of clusters from 2 to 30\n",
        "for n in range(2, 31):\n",
        "    kmeans = KMeans(n_clusters=n, random_state=42, n_init='auto')\n",
        "    labels = kmeans.fit_predict(reduced_embeddings)\n",
        "    sil = silhouette_score(reduced_embeddings, labels)\n",
        "    sil_scores.append(sil)\n",
        "\n",
        "# Best number of clusters based on silhouette score\n",
        "components_range = list(range(2, 31))\n",
        "best_n = components_range[np.argmax(sil_scores)]\n",
        "print(f\"\\nâœ… Best number of clusters based on silhouette score: {best_n}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(components_range, sil_scores, marker='o', linestyle='-', label='Silhouette Score')\n",
        "plt.axvline(x=best_n, color='r', linestyle='--', label=f'Best n = {best_n}')\n",
        "plt.title('Silhouette Scores for KMeans with Different Number of Clusters')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xticks(components_range)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_mBcJwM0wmI"
      },
      "source": [
        "# **Keywords Extraction**\n",
        "\n",
        "This method combines Gaussian Mixture Model clustering with TF-IDF-based keyword extraction to uncover the main themes within a corpus of text data. First, documents are embedded and reduced (using BERT + autoencoder), and then clustered into semantically coherent groups using GMM. After clustering, a TfidfVectorizer is applied to the original text to identify the most representative terms for each cluster. By averaging TF-IDF scores within each group, the top keywords are extractedâ€”offering interpretable insights into the semantic focus of each cluster. The goal is to automatically group and summarize documents by topic without manual labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjyf9fVoT4_K",
        "outputId": "9bc39fcc-40d8-473a-d2c8-175a70483a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹Cluster 0 â€” Top Keywords:\n",
            "iot, wireless, learning, devices, edge, federated, data, communication, fl, energy\n",
            "\n",
            "ðŸ”¹Cluster 1 â€” Top Keywords:\n",
            "quantum, materials, machine, learning, clusters, chemical, density, molecular, energy, model\n",
            "\n",
            "ðŸ”¹Cluster 2 â€” Top Keywords:\n",
            "speech, audio, recognition, neural, acoustic, model, networks, speaker, recurrent, models\n",
            "\n",
            "ðŸ”¹Cluster 3 â€” Top Keywords:\n",
            "image, learning, object, network, convolutional, transformer, attention, vision, supervised, deep\n",
            "\n",
            "ðŸ”¹Cluster 4 â€” Top Keywords:\n",
            "learning, data, classification, vector, kernel, support, feature, algorithm, analysis, sparse\n",
            "\n",
            "ðŸ”¹Cluster 5 â€” Top Keywords:\n",
            "detection, anomaly, intrusion, learning, network, based, data, anomalies, deep, traffic\n",
            "\n",
            "ðŸ”¹Cluster 6 â€” Top Keywords:\n",
            "learning, deep, machine, networks, neural, data, training, supervised, algorithms, models\n",
            "\n",
            "ðŸ”¹Cluster 7 â€” Top Keywords:\n",
            "privacy, learning, data, machine, federated, attacks, models, ai, model, training\n",
            "\n",
            "ðŸ”¹Cluster 8 â€” Top Keywords:\n",
            "species, tree, data, plant, regression, forest, classification, soil, trees, models\n",
            "\n",
            "ðŸ”¹Cluster 9 â€” Top Keywords:\n",
            "language, models, model, tasks, bert, text, neural, attention, training, recurrent\n",
            "\n",
            "ðŸ”¹Cluster 10 â€” Top Keywords:\n",
            "forecasting, neural, time, network, series, term, networks, lstm, power, short\n",
            "\n",
            "ðŸ”¹Cluster 11 â€” Top Keywords:\n",
            "online, learning, education, students, student, teaching, research, ai, study, meta\n",
            "\n",
            "ðŸ”¹Cluster 12 â€” Top Keywords:\n",
            "patients, disease, risk, prediction, machine, regression, cancer, heart, clinical, data\n",
            "\n",
            "ðŸ”¹Cluster 13 â€” Top Keywords:\n",
            "ai, fairness, ethical, intelligence, artificial, ethics, systems, algorithmic, machine, fair\n",
            "\n",
            "ðŸ”¹Cluster 14 â€” Top Keywords:\n",
            "language, llms, models, chatgpt, gpt, ai, model, large, llm, reasoning\n",
            "\n",
            "ðŸ”¹Cluster 15 â€” Top Keywords:\n",
            "diffusion, model, models, time, process, reaction, modeling, data, processes, image\n",
            "\n",
            "ðŸ”¹Cluster 16 â€” Top Keywords:\n",
            "face, recognition, emotion, faces, learning, images, pose, features, human, deep\n",
            "\n",
            "ðŸ”¹Cluster 17 â€” Top Keywords:\n",
            "brain, eeg, memory, attention, neural, learning, mechanisms, cognitive, networks, cortex\n",
            "\n",
            "ðŸ”¹Cluster 18 â€” Top Keywords:\n",
            "image, generative, gan, adversarial, images, gans, models, networks, model, diffusion\n",
            "\n",
            "ðŸ”¹Cluster 19 â€” Top Keywords:\n",
            "topic, sentiment, text, topics, document, classification, model, twitter, lda, models\n",
            "\n",
            "ðŸ”¹Cluster 20 â€” Top Keywords:\n",
            "graph, graphs, learning, node, networks, supervised, data, representation, gnns, network\n",
            "\n",
            "ðŸ”¹Cluster 21 â€” Top Keywords:\n",
            "regression, linear, models, model, logistic, analysis, data, variables, effects, variable\n",
            "\n",
            "ðŸ”¹Cluster 22 â€” Top Keywords:\n",
            "medical, ai, learning, deep, clinical, intelligence, artificial, data, healthcare, machine\n",
            "\n",
            "ðŸ”¹Cluster 23 â€” Top Keywords:\n",
            "landslide, machine, regression, mapping, logistic, using, vector, support, fault, models\n",
            "\n",
            "ðŸ”¹Cluster 24 â€” Top Keywords:\n",
            "cell, gene, protein, data, analysis, cells, expression, cancer, genes, methods\n",
            "\n",
            "ðŸ”¹Cluster 25 â€” Top Keywords:\n",
            "decision, tree, classification, trees, data, machine, learning, algorithms, mining, ensemble\n",
            "\n",
            "ðŸ”¹Cluster 26 â€” Top Keywords:\n",
            "research, model, ai, social, business, technology, theory, study, management, innovation\n",
            "\n",
            "ðŸ”¹Cluster 27 â€” Top Keywords:\n",
            "reinforcement, learning, rl, policy, agent, tasks, deep, control, policies, agents\n",
            "\n",
            "ðŸ”¹Cluster 28 â€” Top Keywords:\n",
            "clustering, means, algorithm, data, cluster, clusters, algorithms, density, hierarchical, based\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Fit Gaussian Mixture Model on the reduced embeddings\n",
        "KMeans = KMeans(n_clusters=best_n, random_state=42)\n",
        "KMeans_labels = KMeans.fit_predict(reduced_embeddings)\n",
        "\n",
        "# Assign cluster labels to the DataFrame\n",
        "df[\"Cluster\"] = KMeans_labels\n",
        "\n",
        "# Initialize TF-IDF vectorizer (ignoring common English stopwords and limiting features to 2000)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\n",
        "\n",
        "# Transform the original text data into a TF-IDF matrix\n",
        "X_tfidf = vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "# Get the list of feature names (The words in the TF-IDF vocab)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "top_n = 10  # Number of top keywords to extract per cluster\n",
        "cluster_keywords = {}\n",
        "\n",
        "# Loop over each cluster to find top keywords\n",
        "for cluster_num in sorted(df[\"Cluster\"].unique()):\n",
        "    # Get the indices of documents in the current cluster\n",
        "    cluster_indices = df[df[\"Cluster\"] == cluster_num].index.to_list()\n",
        "\n",
        "    # Compute the mean TF-IDF score for each word across all docs in the cluster\n",
        "    cluster_tfidf = X_tfidf[cluster_indices].mean(axis=0)\n",
        "\n",
        "    # Convert sparse matrix to a flat NumPy array\n",
        "    cluster_array = np.squeeze(np.asarray(cluster_tfidf))\n",
        "\n",
        "    # Get indices of the top n highest scoring words\n",
        "    top_term_indices = cluster_array.argsort()[::-1][:top_n]\n",
        "\n",
        "    # Retrieve the actual words corresponding to those indices\n",
        "    keywords = [terms[i] for i in top_term_indices]\n",
        "\n",
        "    # Store the keywords for this cluster\n",
        "    cluster_keywords[cluster_num] = keywords\n",
        "\n",
        "# Print the top keywords for each cluster\n",
        "for cluster, keywords in cluster_keywords.items():\n",
        "    print(f\"\\nðŸ”¹Cluster {cluster} â€” Top Keywords:\")\n",
        "    print(\", \".join(keywords))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVjLBZboWq5W",
        "outputId": "89143463-c6fe-471f-abec-b18bbf8493ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Cluster 0 has 1759 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 1 has 1809 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 2 has 1796 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 3 has 1949 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 4 has 1861 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 5 has 1876 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 6 has 1945 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 7 has 1842 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 8 has 1822 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 9 has 1907 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 10 has 1847 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 11 has 1662 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 12 has 1819 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 13 has 1552 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 14 has 1834 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 15 has 1783 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 16 has 1786 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 17 has 1862 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 18 has 1868 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 19 has 1828 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 20 has 1780 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 21 has 1786 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 22 has 1920 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 23 has 1592 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 24 has 1845 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 25 has 1759 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 26 has 1658 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 27 has 1868 non-zero TF-IDF keywords.\n",
            "ðŸ”¹ Cluster 28 has 1755 non-zero TF-IDF keywords.\n"
          ]
        }
      ],
      "source": [
        "# Count how many non-zero TF-IDF terms each cluster has\n",
        "for cluster_num in sorted(df[\"Cluster\"].unique()):\n",
        "    cluster_indices = df[df[\"Cluster\"] == cluster_num].index.to_list()\n",
        "    cluster_tfidf = X_tfidf[cluster_indices].mean(axis=0)\n",
        "    cluster_array = np.squeeze(np.asarray(cluster_tfidf))\n",
        "\n",
        "    nonzero_count = np.count_nonzero(cluster_array)\n",
        "    print(f\"ðŸ”¹ Cluster {cluster_num} has {nonzero_count} non-zero TF-IDF keywords.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbn72PJxR3s"
      },
      "source": [
        "# **Implementatiom**\n",
        "\n",
        "The following method combines semantic embeddings, dimensionality reduction, and hierarchical clustering to retrieve relevant documents based on a user's query. First, each document is encoded using a Sentence-BERT model to capture deep semantic meaning. These high-dimensional embeddings are then compressed using an autoencoder, which reduces noise and preserves core structure. A Gaussian Mixture Model is applied to the reduced embeddings to group documents into high-level semantic clusters. When a user submits a query, it undergoes the same encoding and reduction process, is assigned to a cluster, and is compared to the cluster's documents using cosine similarity. A second layer of GMM clustering is then applied to document titles to refine the results even further, ensuring that the final recommendations are both thematically and topically relevant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYYBM8YfkWgW",
        "outputId": "47ae8742-d751-482e-aa19-818f43aef47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter keywords to search: pca\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 962 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dbd48e47920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "encoder = load_model('/content/drive/MyDrive/encoder_model.h5')\n",
        "df[\"Embeddings\"] = list(embeddings)  # Assign precomputed embeddings to the DataFrame\n",
        "\n",
        "# First-level clustering using reduced embeddings from the autoencoder\n",
        "kmeans_model = KMeans(n_clusters=best_n, n_init='auto', random_state=42)\n",
        "df[\"Cluster\"] = kmeans_model.fit_predict(reduced_embeddings)  # Assign cluster labels to DataFrame\n",
        "\n",
        "def search_similar_resources(query, df, top_n=10, min_similarity=0.50, title_cluster_k=3):\n",
        "    if not query.strip():  # Check for empty query\n",
        "        print(\"Query is empty. Please enter valid keywords.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Encode and reduce query using the trained SentenceTransformer and autoencoder\n",
        "    query_embedding = model.encode([query])  # Convert query to embedding\n",
        "    query_embedding_reduced = encoder.predict(query_embedding)  # Reduce embedding using autoencoder\n",
        "\n",
        "    # Predict which cluster the query belongs to using the KMeans\n",
        "    query_cluster = kmeans_model.predict(query_embedding_reduced)[0]\n",
        "    cluster_df = df[df[\"Cluster\"] == query_cluster].copy()  # Filter samples in the same cluster\n",
        "\n",
        "    if cluster_df.empty:\n",
        "        print(\"No relevant resources found in the identified cluster.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Compute cosine similarity between query and cluster embeddings\n",
        "    cluster_embeddings = np.vstack(cluster_df[\"Embeddings\"].values)\n",
        "    similarity_scores = cosine_similarity(query_embedding, cluster_embeddings).flatten()\n",
        "    cluster_df[\"Similarity\"] = similarity_scores\n",
        "    cluster_df = cluster_df[cluster_df[\"Similarity\"] >= min_similarity]\n",
        "\n",
        "    if cluster_df.empty:\n",
        "        print(\"No resources met the minimum similarity threshold.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Title-level clustering: group similar titles within the filtered cluster\n",
        "    title_embeddings = model.encode(cluster_df[\"Title\"].tolist())\n",
        "    if len(title_embeddings) < title_cluster_k:\n",
        "        title_cluster_k = max(1, len(title_embeddings))  # Avoid KMeans crash\n",
        "\n",
        "    title_kmeans = KMeans(n_clusters=title_cluster_k, random_state=42)\n",
        "    cluster_df[\"TitleCluster\"] = title_kmeans.fit_predict(title_embeddings)\n",
        "\n",
        "    # Predict which title sub-cluster the query belongs to\n",
        "    query_title_embedding = model.encode([query])\n",
        "    query_title_cluster = title_kmeans.predict(query_title_embedding)[0]\n",
        "    subcluster_df = cluster_df[cluster_df[\"TitleCluster\"] == query_title_cluster].copy()\n",
        "\n",
        "    if subcluster_df.empty:\n",
        "        print(\"No relevant titles found in the identified subcluster.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    subcluster_df = subcluster_df.drop_duplicates(subset=[\"Title\", \"Similarity\"])\n",
        "    top_results = subcluster_df.sort_values(by=\"Similarity\", ascending=False).head(top_n)\n",
        "    return top_results[[\"Title\", \"URL\", \"Similarity\"]]\n",
        "\n",
        "# Example usage\n",
        "query = input(\"Enter keywords to search: \")\n",
        "recommendations = search_similar_resources(query, df, top_n=10, min_similarity=0.50, title_cluster_k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lihX8WS60clf"
      },
      "source": [
        "# **Print Out The Recommended Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jfQZEyEt4e2k",
        "outputId": "d9abb4c7-2011-486e-8a70-99ec13f61cdb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ðŸ” Search Results:\n- **Title:** [A Tutorial on Principal Component Analysis](https://www.semanticscholar.org/paper/562e7f497eff8363825abad8d0008a42ce00eb49)\n  - â­ **Similarity Score:** 0.67\n\n- **Title:** [Principal Component Analysis](https://www.semanticscholar.org/paper/ae7b96d287893d246313ccd0566cc4a17f863d44)\n  - â­ **Similarity Score:** 0.67\n\n- **Title:** [An overview of principal component analysis](https://www.semanticscholar.org/paper/5939cb4e961dcab19cfb1d529e82dc872e95a694)\n  - â­ **Similarity Score:** 0.66\n\n- **Title:** [On Consistency and Sparsity for Principal Components Analysis in High Dimensions](https://www.semanticscholar.org/paper/6b0345fe5dbf7a8551edd7ae3f56f803fc21378a)\n  - â­ **Similarity Score:** 0.57\n\n- **Title:** [Applying Principal Components Analysis to Event-Related Potentials: A Tutorial](https://www.semanticscholar.org/paper/db7754609cadd6b4453b8a1a4cbe3ee99709dde0)\n  - â­ **Similarity Score:** 0.56\n\n- **Title:** [A Randomized Algorithm for Principal Component Analysis](https://www.semanticscholar.org/paper/08a757bb53efafbb0eec6cd0a9ab3f128b6d01d3)\n  - â­ **Similarity Score:** 0.55\n\n- **Title:** [Finite sample approximation results for principal component analysis: a matrix perturbation approach](https://www.semanticscholar.org/paper/7c0f5b508303c35da77fa775fa08fdfa5f6b37db)\n  - â­ **Similarity Score:** 0.53\n\n- **Title:** [iPCA: An Interactive System for PCAâ€based Visual Analytics](https://www.semanticscholar.org/paper/1b95578b8b50d5de940f2e85a6126f63c6c04ff0)\n  - â­ **Similarity Score:** 0.53\n\n- **Title:** [Independent Vector Analysis: An Extension of ICA to Multivariate Components](https://www.semanticscholar.org/paper/ad26804e1c70d75fd22552f18ba5f84bf592591d)\n  - â­ **Similarity Score:** 0.52\n\n- **Title:** [Sparse Principal Components Analysis](https://www.semanticscholar.org/paper/39ac586c519ef8569b4f401a257c29b2039d5d25)\n  - â­ **Similarity Score:** 0.52\n\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, Markdown\n",
        "\n",
        "def display_results_as_markdown(results):\n",
        "    \"\"\"\n",
        "    Display search results as Markdown in Jupyter Notebook.\n",
        "    \"\"\"\n",
        "    output = \"### ðŸ” Search Results:\\n\"\n",
        "    for index, row in recommendations.iterrows():\n",
        "        output += f\"- **Title:** [{row['Title']}]({row['URL']})\\n\"\n",
        "        output += f\"  - â­ **Similarity Score:** {row['Similarity']:.2f}\\n\\n\"\n",
        "    display(Markdown(output))\n",
        "\n",
        "# Call function\n",
        "display_results_as_markdown(recommendations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3gP0DN7HKe",
        "outputId": "6fe1665e-ff9f-4268-cd68-1002bf1d21c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/embeddings.npy\", embeddings)\n",
        "encoder.save('/content/drive/MyDrive/encoder_model.h5')\n",
        "np.save(\"/content/drive/MyDrive/reduced_embeddings.npy\", reduced_embeddings)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}