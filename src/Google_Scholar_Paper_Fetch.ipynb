{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voyY5fDTABju"
      },
      "outputs": [],
      "source": [
        "!pip install requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "XBnzC_ASX3DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlS4tLvk_8on"
      },
      "outputs": [],
      "source": [
        "import requests  # For making API requests\n",
        "import pandas as pd  #\n",
        "import time  # For handling API rate limits\n",
        "import os  # For accessing environment variables\n",
        "from google.colab import files  # For downloading files in Google Colab\n",
        "from datetime import datetime  # For handling date operations\n",
        "from langdetect import detect, LangDetectException # detect = language identifier, LangDetectException = error if detection fails\n",
        "\n",
        "# Load API key from an environment variable for security\n",
        "SEMANTIC_SCHOLAR_API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\", \"Inset API Key\")\n",
        "\n",
        "# Get the current year and define the past 20 years range\n",
        "CURRENT_YEAR = datetime.now().year  # Get the current year\n",
        "START_YEAR = CURRENT_YEAR - 20  # Define the start year as 20 years ago\n",
        "\n",
        "# Function to fetch AI/ML research papers from Semantic Scholar with pagination\n",
        "def fetch_research_papers(query, max_results=100):  # Allows fetching up to 100 results per query\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"  # API endpoint for research papers\n",
        "    headers = {\"x-api-key\": SEMANTIC_SCHOLAR_API_KEY}  # API authentication header\n",
        "\n",
        "    papers = []  # List to store fetched research papers\n",
        "    total_fetched = 0  # Counter to track the total number of papers fetched\n",
        "    offset = 0  # Track pagination to avoid duplicate results\n",
        "\n",
        "    # Loop until we fetch the required number of results\n",
        "    while total_fetched < max_results:\n",
        "        params = {\n",
        "            \"query\": query,  # Search query term\n",
        "            \"limit\": min(100, max_results - total_fetched),  # Fetch up to 100 at a time (Predefined by Google)\n",
        "            \"offset\": offset,  # Pagination offset to fetch next batch of results\n",
        "            \"fields\": \"title,abstract,authors,year,url,citationCount,journal,venue,publicationTypes\", # Lables of dataset\n",
        "            \"year\": f\"{START_YEAR}-{CURRENT_YEAR}\"  # Fetch only from the last 20 years\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, params=params)  # Make API request\n",
        "\n",
        "        # Handle API rate limits and errors with exponential backoff\n",
        "        if response.status_code == 429:  # If API rate limit is exceeded\n",
        "            wait_time = 2  # Initial wait time in seconds\n",
        "            while response.status_code == 429:  # Keep retrying until allowed\n",
        "                print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)  # Wait before retrying\n",
        "                wait_time *= 2  # Increase wait time (exponential backoff)\n",
        "                response = requests.get(url, headers=headers, params=params)  # Retry API request\n",
        "\n",
        "        elif response.status_code != 200:  # Handle other errors\n",
        "            print(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
        "            break  # Stop execution on API error\n",
        "\n",
        "        data = response.json()  # Convert API response to JSON format\n",
        "        papers_fetched = data.get(\"data\", [])  # Extract paper data from response\n",
        "\n",
        "        # If no more results, stop fetching\n",
        "        if not papers_fetched:\n",
        "            print(f\"No more results for query: {query}\")\n",
        "            break\n",
        "\n",
        "        for paper in papers_fetched:\n",
        "            year = paper.get(\"year\", 0)\n",
        "            if START_YEAR <= year <= CURRENT_YEAR:\n",
        "               title = paper.get(\"title\", \"\")\n",
        "               abstract = paper.get(\"abstract\", \"\")\n",
        "\n",
        "            try:\n",
        "               if detect(title) != \"en\":\n",
        "                    continue\n",
        "               if abstract and detect(abstract) != \"en\":\n",
        "                continue\n",
        "            except LangDetectException:\n",
        "                continue  # skip if language detection fails\n",
        "\n",
        "            papers.append({\n",
        "                \"Title\": title or \"N/A\",\n",
        "                \"Abstract\": abstract or \"N/A\",\n",
        "                \"Authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])]),\n",
        "                \"Year\": year,\n",
        "                \"URL\": paper.get(\"url\", \"N/A\"),\n",
        "                \"Citations\": paper.get(\"citationCount\", \"N/A\"),\n",
        "                \"Journal\": (paper.get(\"journal\") or {}).get(\"name\", \"N/A\"),\n",
        "                \"Venue\": paper.get(\"venue\", \"N/A\"),\n",
        "                \"Publication Types\": \", \".join(paper.get(\"publicationTypes\", []) or [])\n",
        "            })\n",
        "\n",
        "        total_fetched += len(papers_fetched)  # Update the total fetched count\n",
        "        offset += len(papers_fetched)  # Move the offset forward for pagination\n",
        "\n",
        "        # Respect API rate limits by adding a small delay\n",
        "        time.sleep(1)\n",
        "\n",
        "    return papers  # Return the list of fetched papers\n",
        "\n",
        "# List of queries to fetch AI & ML research papers\n",
        "queries = [\n",
        "    # Machine Learning Variants\n",
        "    \"Machine Learning\",\n",
        "    \"Machine Learning in Healthcare\",\n",
        "    \"Machine Learning in Finance\",\n",
        "    \"Machine Learning in Cybersecurity\",\n",
        "    \"Automated Machine Learning (AutoML)\",\n",
        "    \"Bayesian Machine Learning\",\n",
        "\n",
        "    # Deep Learning\n",
        "    \"Deep Learning\",\n",
        "    \"Deep Learning Architectures\",\n",
        "    \"Deep Learning for Natural Language Processing\",\n",
        "    \"Deep Learning for Computer Vision\",\n",
        "    \"Neural Network Optimization\",\n",
        "    \"Deep Reinforcement Learning\",\n",
        "\n",
        "    # Reinforcement Learning\n",
        "    \"Reinforcement Learning\",\n",
        "    \"Multi-Agent Reinforcement Learning\",\n",
        "    \"Q-Learning\",\n",
        "    \"Policy Gradient Methods\",\n",
        "    \"Model-Based Reinforcement Learning\",\n",
        "    \"Self-Play in Reinforcement Learning\",\n",
        "\n",
        "    # Natural Language Processing (NLP)\n",
        "    \"Natural Language Processing\",\n",
        "    \"Transformer Models for NLP\",\n",
        "    \"BERT Model\",\n",
        "    \"GPT Models\",\n",
        "    \"Language Modeling\",\n",
        "    \"Sentiment Analysis\",\n",
        "\n",
        "    # Computer Vision\n",
        "    \"Computer Vision\",\n",
        "    \"Image Recognition\",\n",
        "    \"Object Detection\",\n",
        "    \"Face Recognition\",\n",
        "    \"3D Computer Vision\",\n",
        "    \"Medical Image Analysis\",\n",
        "\n",
        "    # AI Architectures & Networks\n",
        "    \"Neural Networks\",\n",
        "    \"Graph Neural Networks\",\n",
        "    \"Recurrent Neural Networks (RNN)\",\n",
        "    \"Convolutional Neural Networks (CNN)\",\n",
        "    \"Transformer Networks\",\n",
        "\n",
        "    # Unsupervised & Self-Supervised Learning\n",
        "    \"Unsupervised Learning\",\n",
        "    \"Self-Supervised Learning\",\n",
        "    \"Contrastive Learning\",\n",
        "    \"Representation Learning\",\n",
        "    \"Few-Shot Learning\",\n",
        "    \"Zero-Shot Learning\",\n",
        "\n",
        "    # AI Applications\n",
        "    \"AI in Drug Discovery\",\n",
        "    \"AI for Autonomous Vehicles\",\n",
        "    \"AI in Agriculture\",\n",
        "    \"AI in Finance\",\n",
        "    \"AI in Robotics\",\n",
        "    \"AI for Edge Computing\",\n",
        "\n",
        "    # Ethical AI\n",
        "    \"Explainable AI\",\n",
        "    \"AI Ethics\",\n",
        "    \"Fairness in AI\",\n",
        "    \"Bias in AI Models\",\n",
        "\n",
        "    # Generative AI\n",
        "    \"Generative Adversarial Networks (GANs)\",\n",
        "    \"Stable Diffusion Models\",\n",
        "    \"Text-to-Image AI\",\n",
        "    \"AI Art Generation\",\n",
        "\n",
        "    # Big Data & AI\n",
        "    \"Big Data Analytics\",\n",
        "    \"AI for Big Data\",\n",
        "    \"Federated Learning\",\n",
        "\n",
        "    # AI in Forecasting & Predictions\n",
        "    \"Time Series Forecasting\",\n",
        "    \"Predictive Analytics\",\n",
        "    \"Anomaly Detection\",\n",
        "    \"Clustering Algorithms\",\n",
        "\n",
        "    # Emerging AI Fields\n",
        "    \"Neuro-Symbolic AI\",\n",
        "    \"Multi-Modal AI\",\n",
        "    \"AI for Scientific Research\",\n",
        "    \"AI in Quantum Computing\",\n",
        "    \"AI for IoT\",\n",
        "    \"AI Hardware Acceleration\"\n",
        "]\n",
        "\n",
        "all_papers = []  # List to store papers from all queries\n",
        "\n",
        "# Loop through each query and fetch research papers\n",
        "for query in queries:\n",
        "    print(f\"Fetching up to 100 papers for: {query}\")\n",
        "    papers = fetch_research_papers(query, max_results=100)  # Fetch papers for the given query\n",
        "    all_papers.extend(papers)  # Append fetched papers to the main list\n",
        "\n",
        "# Convert fetched data to a Pandas DataFrame\n",
        "df = pd.DataFrame(all_papers)\n",
        "\n",
        "# Save data as a CSV file\n",
        "csv_filename = \"Google Scholar AI&ML Papers.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Download the file (for Google Colab)\n",
        "files.download(csv_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}