{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "voyY5fDTABju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a91bc8-1b20-4df0-966a-8562e1769e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBnzC_ASX3DI",
        "outputId": "4510a447-0b18-4d31-f663-0c2a4cb0bf89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/981.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=7622ccb1bd5909fbafd9483c6df790e0400d32148f177bf9ba021d6b9ec3dd20\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BlS4tLvk_8on",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f8b8fec-52d1-4907-9708-2ca08528f9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching up to 100 papers for: Machine Learning\n",
            "Fetching up to 100 papers for: Machine Learning in Healthcare\n",
            "Fetching up to 100 papers for: Machine Learning in Finance\n",
            "Fetching up to 100 papers for: Machine Learning in Cybersecurity\n",
            "Fetching up to 100 papers for: Automated Machine Learning (AutoML)\n",
            "Fetching up to 100 papers for: Bayesian Machine Learning\n",
            "Fetching up to 100 papers for: Deep Learning\n",
            "Fetching up to 100 papers for: Deep Learning Architectures\n",
            "Fetching up to 100 papers for: Deep Learning for Natural Language Processing\n",
            "Fetching up to 100 papers for: Deep Learning for Computer Vision\n",
            "Fetching up to 100 papers for: Neural Network Optimization\n",
            "Fetching up to 100 papers for: Deep Reinforcement Learning\n",
            "Fetching up to 100 papers for: Reinforcement Learning\n",
            "Fetching up to 100 papers for: Multi-Agent Reinforcement Learning\n",
            "Fetching up to 100 papers for: Q-Learning\n",
            "Fetching up to 100 papers for: Policy Gradient Methods\n",
            "Fetching up to 100 papers for: Model-Based Reinforcement Learning\n",
            "Fetching up to 100 papers for: Self-Play in Reinforcement Learning\n",
            "Fetching up to 100 papers for: Natural Language Processing\n",
            "Fetching up to 100 papers for: Transformer Models for NLP\n",
            "Fetching up to 100 papers for: BERT Model\n",
            "Fetching up to 100 papers for: GPT Models\n",
            "Fetching up to 100 papers for: Language Modeling\n",
            "Fetching up to 100 papers for: Sentiment Analysis\n",
            "Fetching up to 100 papers for: Computer Vision\n",
            "Fetching up to 100 papers for: Image Recognition\n",
            "Fetching up to 100 papers for: Object Detection\n",
            "Fetching up to 100 papers for: Face Recognition\n",
            "Fetching up to 100 papers for: 3D Computer Vision\n",
            "Fetching up to 100 papers for: Medical Image Analysis\n",
            "Fetching up to 100 papers for: Neural Networks\n",
            "Fetching up to 100 papers for: Graph Neural Networks\n",
            "Fetching up to 100 papers for: Recurrent Neural Networks (RNN)\n",
            "Fetching up to 100 papers for: Convolutional Neural Networks (CNN)\n",
            "Fetching up to 100 papers for: Transformer Networks\n",
            "Fetching up to 100 papers for: Unsupervised Learning\n",
            "Fetching up to 100 papers for: Self-Supervised Learning\n",
            "Fetching up to 100 papers for: Contrastive Learning\n",
            "Fetching up to 100 papers for: Representation Learning\n",
            "Fetching up to 100 papers for: Few-Shot Learning\n",
            "Fetching up to 100 papers for: Zero-Shot Learning\n",
            "Fetching up to 100 papers for: AI in Drug Discovery\n",
            "Fetching up to 100 papers for: AI for Autonomous Vehicles\n",
            "Fetching up to 100 papers for: AI in Agriculture\n",
            "Fetching up to 100 papers for: AI in Finance\n",
            "Fetching up to 100 papers for: AI in Robotics\n",
            "Fetching up to 100 papers for: AI for Edge Computing\n",
            "Fetching up to 100 papers for: Explainable AI\n",
            "Fetching up to 100 papers for: AI Ethics\n",
            "Fetching up to 100 papers for: Fairness in AI\n",
            "Fetching up to 100 papers for: Bias in AI Models\n",
            "Fetching up to 100 papers for: Generative Adversarial Networks (GANs)\n",
            "Fetching up to 100 papers for: Stable Diffusion Models\n",
            "Fetching up to 100 papers for: Text-to-Image AI\n",
            "Fetching up to 100 papers for: AI Art Generation\n",
            "Fetching up to 100 papers for: Big Data Analytics\n",
            "Fetching up to 100 papers for: AI for Big Data\n",
            "Fetching up to 100 papers for: Federated Learning\n",
            "Fetching up to 100 papers for: Time Series Forecasting\n",
            "Fetching up to 100 papers for: Predictive Analytics\n",
            "Fetching up to 100 papers for: Anomaly Detection\n",
            "Fetching up to 100 papers for: Clustering Algorithms\n",
            "Fetching up to 100 papers for: Neuro-Symbolic AI\n",
            "Fetching up to 100 papers for: Multi-Modal AI\n",
            "Fetching up to 100 papers for: AI for Scientific Research\n",
            "Fetching up to 100 papers for: AI in Quantum Computing\n",
            "Fetching up to 100 papers for: AI for IoT\n",
            "Fetching up to 100 papers for: AI Hardware Acceleration\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_42205795-70bb-4556-bd1f-11f7e79091b9\", \"Google Scholar AI&ML Papers.csv\", 9454204)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests  # For making API requests\n",
        "import pandas as pd  #\n",
        "import time  # For handling API rate limits\n",
        "import os  # For accessing environment variables\n",
        "from google.colab import files  # For downloading files in Google Colab\n",
        "from datetime import datetime  # For handling date operations\n",
        "from langdetect import detect, LangDetectException # detect = language identifier, LangDetectException = error if detection fails\n",
        "\n",
        "# Load API key from an environment variable for security\n",
        "SEMANTIC_SCHOLAR_API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\", \"Inset API Key\")\n",
        "\n",
        "# Get the current year and define the past 20 years range\n",
        "CURRENT_YEAR = datetime.now().year  # Get the current year\n",
        "START_YEAR = CURRENT_YEAR - 20  # Define the start year as 20 years ago\n",
        "\n",
        "# Function to fetch AI/ML research papers from Semantic Scholar with pagination\n",
        "def fetch_research_papers(query, max_results=100):  # Allows fetching up to 100 results per query\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"  # API endpoint for research papers\n",
        "    headers = {\"x-api-key\": SEMANTIC_SCHOLAR_API_KEY}  # API authentication header\n",
        "\n",
        "    papers = []  # List to store fetched research papers\n",
        "    total_fetched = 0  # Counter to track the total number of papers fetched\n",
        "    offset = 0  # Track pagination to avoid duplicate results\n",
        "\n",
        "    # Loop until we fetch the required number of results\n",
        "    while total_fetched < max_results:\n",
        "        params = {\n",
        "            \"query\": query,  # Search query term\n",
        "            \"limit\": min(100, max_results - total_fetched),  # Fetch up to 100 at a time (Predefined by Google)\n",
        "            \"offset\": offset,  # Pagination offset to fetch next batch of results\n",
        "            \"fields\": \"title,abstract,authors,year,url,citationCount,journal,venue,publicationTypes\", # Lables of dataset\n",
        "            \"year\": f\"{START_YEAR}-{CURRENT_YEAR}\"  # Fetch only from the last 20 years\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, params=params)  # Make API request\n",
        "\n",
        "        # Handle API rate limits and errors with exponential backoff\n",
        "        if response.status_code == 429:  # If API rate limit is exceeded\n",
        "            wait_time = 2  # Initial wait time in seconds\n",
        "            while response.status_code == 429:  # Keep retrying until allowed\n",
        "                print(f\"Rate limit exceeded. Waiting for {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)  # Wait before retrying\n",
        "                wait_time *= 2  # Increase wait time (exponential backoff)\n",
        "                response = requests.get(url, headers=headers, params=params)  # Retry API request\n",
        "\n",
        "        elif response.status_code != 200:  # Handle other errors\n",
        "            print(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
        "            break  # Stop execution on API error\n",
        "\n",
        "        data = response.json()  # Convert API response to JSON format\n",
        "        papers_fetched = data.get(\"data\", [])  # Extract paper data from response\n",
        "\n",
        "        # If no more results, stop fetching\n",
        "        if not papers_fetched:\n",
        "            print(f\"No more results for query: {query}\")\n",
        "            break\n",
        "\n",
        "        for paper in papers_fetched:\n",
        "            year = paper.get(\"year\", 0)\n",
        "            if START_YEAR <= year <= CURRENT_YEAR:\n",
        "               title = paper.get(\"title\", \"\")\n",
        "               abstract = paper.get(\"abstract\", \"\")\n",
        "\n",
        "            try:\n",
        "               if detect(title) != \"en\":\n",
        "                    continue\n",
        "               if abstract and detect(abstract) != \"en\":\n",
        "                continue\n",
        "            except LangDetectException:\n",
        "                continue  # skip if language detection fails\n",
        "\n",
        "            papers.append({\n",
        "                \"Title\": title or \"N/A\",\n",
        "                \"Abstract\": abstract or \"N/A\",\n",
        "                \"Authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])]),\n",
        "                \"Year\": year,\n",
        "                \"URL\": paper.get(\"url\", \"N/A\"),\n",
        "                \"Citations\": paper.get(\"citationCount\", \"N/A\"),\n",
        "                \"Journal\": (paper.get(\"journal\") or {}).get(\"name\", \"N/A\"),\n",
        "                \"Venue\": paper.get(\"venue\", \"N/A\"),\n",
        "                \"Publication Types\": \", \".join(paper.get(\"publicationTypes\", []) or [])\n",
        "            })\n",
        "\n",
        "        total_fetched += len(papers_fetched)  # Update the total fetched count\n",
        "        offset += len(papers_fetched)  # Move the offset forward for pagination\n",
        "\n",
        "        # Respect API rate limits by adding a small delay\n",
        "        time.sleep(1)\n",
        "\n",
        "    return papers  # Return the list of fetched papers\n",
        "\n",
        "# List of queries to fetch AI & ML research papers\n",
        "queries = [\n",
        "    # Machine Learning Variants\n",
        "    \"Machine Learning\",\n",
        "    \"Machine Learning in Healthcare\",\n",
        "    \"Machine Learning in Finance\",\n",
        "    \"Machine Learning in Cybersecurity\",\n",
        "    \"Automated Machine Learning (AutoML)\",\n",
        "    \"Bayesian Machine Learning\",\n",
        "\n",
        "    # Deep Learning\n",
        "    \"Deep Learning\",\n",
        "    \"Deep Learning Architectures\",\n",
        "    \"Deep Learning for Natural Language Processing\",\n",
        "    \"Deep Learning for Computer Vision\",\n",
        "    \"Neural Network Optimization\",\n",
        "    \"Deep Reinforcement Learning\",\n",
        "\n",
        "    # Reinforcement Learning\n",
        "    \"Reinforcement Learning\",\n",
        "    \"Multi-Agent Reinforcement Learning\",\n",
        "    \"Q-Learning\",\n",
        "    \"Policy Gradient Methods\",\n",
        "    \"Model-Based Reinforcement Learning\",\n",
        "    \"Self-Play in Reinforcement Learning\",\n",
        "\n",
        "    # Natural Language Processing (NLP)\n",
        "    \"Natural Language Processing\",\n",
        "    \"Transformer Models for NLP\",\n",
        "    \"BERT Model\",\n",
        "    \"GPT Models\",\n",
        "    \"Language Modeling\",\n",
        "    \"Sentiment Analysis\",\n",
        "\n",
        "    # Computer Vision\n",
        "    \"Computer Vision\",\n",
        "    \"Image Recognition\",\n",
        "    \"Object Detection\",\n",
        "    \"Face Recognition\",\n",
        "    \"3D Computer Vision\",\n",
        "    \"Medical Image Analysis\",\n",
        "\n",
        "    # AI Architectures & Networks\n",
        "    \"Neural Networks\",\n",
        "    \"Graph Neural Networks\",\n",
        "    \"Recurrent Neural Networks (RNN)\",\n",
        "    \"Convolutional Neural Networks (CNN)\",\n",
        "    \"Transformer Networks\",\n",
        "\n",
        "    # Unsupervised & Self-Supervised Learning\n",
        "    \"Unsupervised Learning\",\n",
        "    \"Self-Supervised Learning\",\n",
        "    \"Contrastive Learning\",\n",
        "    \"Representation Learning\",\n",
        "    \"Few-Shot Learning\",\n",
        "    \"Zero-Shot Learning\",\n",
        "\n",
        "    # AI Applications\n",
        "    \"AI in Drug Discovery\",\n",
        "    \"AI for Autonomous Vehicles\",\n",
        "    \"AI in Agriculture\",\n",
        "    \"AI in Finance\",\n",
        "    \"AI in Robotics\",\n",
        "    \"AI for Edge Computing\",\n",
        "\n",
        "    # Ethical AI\n",
        "    \"Explainable AI\",\n",
        "    \"AI Ethics\",\n",
        "    \"Fairness in AI\",\n",
        "    \"Bias in AI Models\",\n",
        "\n",
        "    # Generative AI\n",
        "    \"Generative Adversarial Networks (GANs)\",\n",
        "    \"Stable Diffusion Models\",\n",
        "    \"Text-to-Image AI\",\n",
        "    \"AI Art Generation\",\n",
        "\n",
        "    # Big Data & AI\n",
        "    \"Big Data Analytics\",\n",
        "    \"AI for Big Data\",\n",
        "    \"Federated Learning\",\n",
        "\n",
        "    # AI in Forecasting & Predictions\n",
        "    \"Time Series Forecasting\",\n",
        "    \"Predictive Analytics\",\n",
        "    \"Anomaly Detection\",\n",
        "    \"Clustering Algorithms\",\n",
        "\n",
        "    # Emerging AI Fields\n",
        "    \"Neuro-Symbolic AI\",\n",
        "    \"Multi-Modal AI\",\n",
        "    \"AI for Scientific Research\",\n",
        "    \"AI in Quantum Computing\",\n",
        "    \"AI for IoT\",\n",
        "    \"AI Hardware Acceleration\"\n",
        "]\n",
        "\n",
        "all_papers = []  # List to store papers from all queries\n",
        "\n",
        "# Loop through each query and fetch research papers\n",
        "for query in queries:\n",
        "    print(f\"Fetching up to 100 papers for: {query}\")\n",
        "    papers = fetch_research_papers(query, max_results=100)  # Fetch papers for the given query\n",
        "    all_papers.extend(papers)  # Append fetched papers to the main list\n",
        "\n",
        "# Convert fetched data to a Pandas DataFrame\n",
        "df = pd.DataFrame(all_papers)\n",
        "\n",
        "# Save data as a CSV file\n",
        "csv_filename = \"Google Scholar AI&ML Papers.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Download the file (for Google Colab)\n",
        "files.download(csv_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}